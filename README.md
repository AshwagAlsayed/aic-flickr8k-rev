# aic-flickr8k-rev
## Introduction
Image captioning is becoming increasingly important as visual media is created at an unprecedented rate on the web and social media platforms. Research on image captioning targeting English captions has received significant attention in the past decade (Kiros et al., 2014; Xu et al., 2015; Sharma et al., 2018; Cornia et al., 2020; Dubey et al., 2023). Unfortunately, image captioning for the Arabic language did not receive similar attention, and few existing works focused on Arabic image captioning (AIC). To solve the AIC problem, existing models developed for English can be used to generate captions in English, then use a text-to-text language translation model to get the captions in Arabic. However, there are many drawbacks to this approach. First, captioning inaccuracies may result from different sources, namely the captioning and translation models. In addition, using the model can be slower because it performs two steps to get the Arabic text from the image. Furthermore, generating captions in Arabic faces many challenges. The Arabic language has a vast morphology (22,400 tags compared to 48 tags in English) (Habash, 2010). Moreover, Arabic has a fairly complex inflectional system. For example, Arabic verbs have over 5,400 different inflected forms. Thus, generating meaningful Arabic captions that are grammatically correct is challenging. In addition to the challenges that stem from the inherent properties of the Arabic language, we identify three factors that impact machine learning models used for AIC. These factors are (1) the labels’ textual preprocessing techniques, (2) the datasets used for building AIC models, and (3) the techniques used to extract the features from the input images to generate the output captions.

## A Transformer-based Architecture for AIC
[![The encoder and decoder in the transformer model architecture are shown on the left and right. Each encoder contains two sublayers, feed forward neural network and self-attention. Therefore, the feed-forward layer is applied after the self-attention layer. The decoder includes the same layers as the encoder and a third layer that sits between the two that were just mentioned; this layer is known as the encoder—decoder attention to help the decoder to focus on relevant parts of the input sentence.](https://i.postimg.cc/htcynQyV/Transformer-Architecture2-1.png)](https://postimg.cc/jL3QQ2rj)

Proposed by Vaswani et al. (2017), the transformer model is one of the deep-learning models that can be used for machine translation. It has been used in a wide range of NLP applications Wang et al. (2019); Raganato and Tiedemann (2018). In addition to NLP applications, the transformer model has also been used for image captioning Sharma et al. (2018). We propose a variant based on Sharma et al. (2018).
Our proposed transformer-based architecture is shown in Fig. 1. The input image is translated into a vector with feature embedding using a CNN-based model. Then, the transformer encoder takes in the feature embedding of the image extracted by CNN, $x = (x_1,..., x_n)$. Thus, the image's feature vector $x$ is computed based on the equation.
\
   $x$ = $CNN(i)$
\
$CNN(i)$ in the equation represents an abstraction over any CNN-based model that is used for image feature extraction. 
After computing $x$, $x$ is used as input to the encoder block, which computes $z$ according to the equation.
$z$ = $Encoder(x)$
\
$Encoder(x)$ in the equation represents an abstraction of the set of encoder blocks. The encoder block generally creates an attention-based representation that can find a particular piece of data within a context that could be ‘‘infinitely” huge. The encoder is composed of one or more identical layers in a stack. Each layer has a simple position-wise, completely connected feed-forward network, and a multi-head self-attention layer. Each sub-layer adopts layer normalization and a residual connection. The same dimension of data is output by each sublayer. In our architecture, the encoder block with multi-head attention converts x to the attended visual representation vector followed by feed-forward, which is applied to every attention vector to transform it into a format that the following (encoder or decoder) layer can understand $z = (z_1, ..., z_t)$. $z$ is the output vector of the encoder block and is fed to the decoder block
$Decoder(z,s)$ in Equation \ref{eq:sdec} represents an abstraction of the set of encoder blocks. In our architecture, the decoder block uses $z$ along with the word embedding vector, $s$, of the input image's captions to generate the output sequence $y = (y_1,..., y_m)$. Word embedding is improved by positional encodings to take advantage of order information. $y$ is the output sequence of words of the decoder block according to the equation.

\
In general, the decoder is composed of one or more identical layers in a stack. Each layer has one sub-layer of a fully connected feed-forward network and two sub-layers of multi-head attention mechanisms. Each sub-layer adopts a residual connection and a layer normalization, just like the encoder. The initial multi-head attention sub-layer is adjusted to avoid positions from paying attention to subsequent positions so that the future of the target sequence is hidden when predicting the current position.
\
The word embedding goes through a masked multi-head attention mechanism which is different from a typical multi-head attention mechanism. The decoder block generates the sequence word by word, so it needs to prevent it from conditioning to future tokens. A second multi-headed attention layer takes the outputs of the encoder and the masked multi-headed attention layer to match the encoder's input to the decoder's input, enabling the decoder to select the encoder input that should receive the most attention. Then the output passes through a point-wise feed-forward layer.

## Text Preprocessing for AIC
In this section, we shed light on an important task which is preparing and preprocessing the dataset for model training and testing. While being a crucial element of any AIC machine-learning pipeline, details of the preprocessing step are often not discussed in the literature. In this paper, we study various techniques for text prepossessing for AIC.   

Most of the existing work in AIC preprocesses the Arabic text using standard preprocessing similar to English text. We refer to this as \emph{general preprocessing}. They segment the labels using a segmentation tool used for the English language. This approach may result in semantic errors in training labels. For example, the definite article "ال" (equivalent to \textbf{the} in English) is frequently prefixed to other words. It is not an integral element of that word. For instance, both "حاسوب" and "الحاسوب" (with the "ال" prefix) must be included in the vocabulary, resulting in a substantial quantity of redundancy. To solve this problem, we segment the Arabic labels into their component prefixes, stems, and suffixes, along with other preprocessing steps. In this paper, we develop a unified framework for preprocessing labels in AIC. Then, we use our framework to investigate the effect of using different approaches (i.e., CAMeL Tools Obeid et al., 2020, ArabertPreprocessor Antoun et al., 2020, and Stanza Qi et al., 2020) for preprocessing Arabic text labels on the performance of AIC.

In general, image captioning models take text labels along with images as input to train the model, and this text must be preprocessed and cleaned to reduce the text's noise and vocabulary size. We propose a framework that abstracts the preprocessing steps into three stages. These stages are general preprocessing (GPP), language-specific preprocessing (LSPP), and task-specific preprocessing (TSPP). It is illustrated in Figure \ref{Preprocessing_pipeline} and explained in detail below.

[![Preprocessing pipeline describes the steps involved in preprocessing where GPP is applied, which is the critical step for any language. After that, the processing steps for the Arabic language, which is LSPP, have been applied, and finally, TSPP makes the text fit to the image captioning model.](https://i.postimg.cc/3wpdZWhb/preprocessing-pipeline111-2.png)](https://postimg.cc/nsFFFFCv)

### General Preprocessing
General text preprocessing is a standard step used for text preprocessing in NLP and AI applications. It is suitable for most languages, like lower-casing all the words, removing punctuation, removing URLs, removing stop words, and removing numeric. Some general preprocessing steps are applied, such as eliminating punctuation and numeric, as shown in the first part (General Preprocessing) of Fig. 2.
### Language-specific Preprocessing
Language-specific preprocessing comprises steps that are specific to a language. In our case, the language is Arabic, as shown in the second part of Fig. 2. However, our framework can be used for other languages as well. The morphology of Arabic is especially significant because the language is highly ordered and derivational. Arabic is written from right to left. The Arabic alphabet has 28 letters in total, including Hamza (ء(, and Arabic letters have various shapes depending on where they are in a word (beginning, middle, or end). For instance, the letters ‘‘ع “and ‘‘ك“, as shown in Table 1, have different shapes depending on their position in the word. The complicated concatenative structure of Arabic is responsible for the Arabic language’s lexical sparsity Al-Sallab et al. (2017). Words can take on various shapes while yet having the same meaning. For instance, although the definite article ‘‘ال“, which corresponds to ‘‘the” in English, is usually prefixed to other words, it is not an essential component of those words. So Arabic NLP applications must handle the Arabic language’s complexity. The following are some of the steps that have been taken to preprocess the Arabic text and reduce its complexity.

**Tatweel Removal.** Some Arabic characters are stretched using the symbol known as the tatweel. In informal Arabic, the tatweel symbol is frequently used to express a sentiment or meaning. The tatweel must be eliminated because it generates redundant spellings of the same word [35].

**Unicode Normalization.** Unicode normalization transforms Unicode strings into their conventional form by normalizing them (characters that can be written as a combination of Unicode characters are converted into their decomposed form) [18].

**Orthographic Normalization.** Orthographic normalization is the process of unifying different letter forms or visually comparable letters. For example, all “ء” characters are removed, the letter "ة" is replaced by “ه” and the letter “ى” is replaced by “ي”. Table 2 summarizes the letter normalization scheme.

**Dediacritization.** The elimination of Arabic diacritical marks is known as dediacritization. Most Arabic NLP algorithms eliminate them because they make the data sparser [18]. For instance, the word مَكْتَبَةٌ is converted to مكتبة.

**Morphological Tokenization.** Standard word tokenization is the process of splitting sentences by whitespace. Morphological tokenization is a different kind of tokenization that separates Arabic words into their components, such as prefixes, stems, and suffixes. For example, the word "المكتبة" is rewritten as three tokens "ال", "مكتب", and "ة". The added "+" symbol denotes where the segment was cut off, allowing us to de-segment the text afterward.

### Task-specific Preprocessing
In the task-specific preprocessing stage, the steps that make the text suitable for the image captioning model are applied, as shown in the third part of Figure 2.
**Markup Tokens.** This step adds < start > and < end > tokens to each caption so that the model knows where the caption begins and ends.

**Vectorization.** The TextVectorization step associates a unique integer value with each token. It also unifies the caption vectors’ length to a specific length, cutting off more extended captions, padding the shorter ones with zeros, and then transforming them into a vector of integers.

### Preprocessing Tools for AIC Model Training
In this paper, we aim to study the impact of three preprocessing tools, namely, the CAMeL Tools [18], ArabertPreprocessor [19], and Stanza [20].

#### Preprocessing Tools
This subsection gives a brief overview of each preprocessing tool we used in our study.

##### CAMeL Tools
CAMeL Tools [18] is an open-source Python-based toolkit that is used for preprocessing Arabic text and dialects, morphological modeling, dialect identification, named entity recognition, and sentiment analysis. These utilities are covered by the command-line interfaces (CLIs) and application programming interfaces (APIs) provided by CAMeL Tools. The CAMeL Tools toolkit has several features that make it flexible in use and reuse, modular, high-performance, and easy to use for beginners.

##### ArabertPreprocessor
ArabertPreprocessor is the text preprocessing component of AraBERT which is a transformer-based model designed for Arabic language understanding [19]. It provides multiple functions for Arabic text-processing and uses FARASA [36] for segmentation. The Farasa segmenter [36] is an Arabic word segmenter used to segment text into stems, prefixes, and suffixes. In addition, apply a specific pre/post-fixed indicator "+" to the segmented articles so that users can reattach the segments to the original term.

##### Stanza
Stanza [20] is created by the Stanford NLP Group [37]. It is a set of specific and efficient tools for analyzing the linguistics of various human languages. Tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition are some techniques it contains that can be utilized in a pipeline.

#### Comparison of Preprocessing Tools
Several differences exist between the aforementioned tools. The CAMeL Tools toolkit provides utilities for preprocessing, morphological modeling, dialect identification, Diacritization, named entity recognition, tokenization, and sentiment analysis. ArabertPreprocessor provides features like Diacritization, tatweel removal, and tokenization. Stanza provides multiple features, including MWT expansion, tokenization, lemmatization, morphological features tagging, and named entity recognition. Table 3 compares the availability of features among the three tools. While these tools may contain functions that can be used for various text-processing tasks, we focus our comparison on the features used for AIC.

#### An Example of Tokenization
Word and sentence composition in Arabic is more complex than in other languages, such as English, and it can lead to a very large lexicon size. For example, a complete sentence in other languages (e.g., English) can be replaced by a single word in Arabic. Thus, there is a need to tokenize Arabic sentences to allow better semantic representation. Moreover, a token is described as a sequence of one or more letters (characters) separated by spaces. For non-agglutinative languages like English, this definition works. With the Arabic language, tokenization is challenging due to the rich and complex morphology of Arabic [38]. The prefix "ال", which leads to redundancy in the vocabulary, also a more complicated example of one Arabic word being translated into a complete sentence in English, such as "وسيكتبونها" which is translated into an English sentence (and they will write it). Tokenization breaks down sentences into their morphemes:
\
"و+""سـ+""يـ+""كتب""+ون""+ها"
\
The second word is divided into three parts, the prefixes "+ي" indicates the masculine present tense, then the stem quotes "لتقط", then the suffixes "ون+" indicates the masculine plural. The fourth word is divided into four parts, "+ف" denotes the order, "+ي" indicates the masculine present tense, the stem "قرؤ", and the suffix "ون+" which indicates the masculine plural. Note that the second and fourth words are divided by CAMeL Tools only. As for the third word, the letter "+و" (equivalent to “and” in English), it is separated in all three tools (but in Stanza, the + sign was not added), and in CAMeL Tools the masculine present "+ي" is also separated as in the previous word.

## Datasets for AIC Research
In this section, we go over the datasets used in our study. In our study, we use two datasets. The first one is provided by ElJundi et al. [28] and is denoted as Flickr8k. The second dataset is denoted Flickr8k-Rev, which is a complete revision of Flickr8k. Flickr8k is the first publicly accessible dataset for AIC. It is created by translating the Flickr8k dataset [17]. The English captions are translated using the Google Translate API, then corrected and validated by a professional Arabic translator to avoid translation errors. Figure 3 depicts a few examples from the dataset and the original English captions.
\
Flickr8k is one of two available Arabic datasets for AIC. The other one is based on the MSCOCO dataset [39] (Arabic-COCO) [40]. The Flickr8k dataset has 8,000 images with 24,276 captions. Thus, there are three captions per image. In our revised dataset, Flickr8k-Rev, we extended the number of captions to 40,000 (i.e., five captions per image) and validated them using native Arabic speakers. The Arabic-COCO dataset has 82,783 images and 413,915 captions (5 captions per image). However, the captions in the Arabic-COCO dataset are machine-translated from its original dataset using Google’s Machine Translation API, and human validators do not validate it.
\
The captions in Flickr8k is further validated by human validators after being machine-translated. However, unfortunately, after reviewing the captions in the Flickr8k dataset, we discovered many spelling and grammatical errors. Examples of these errors are shown in Table 5. For example, in the first sentence, we notice a grammatical error in terms of sentence order (subject + verb + object) in the sentence: the object "كرة السلة" precedes the verb "يلعبان". In this case, the correction is to rearrange the sentence:
\
صبيان يرتديان الزي الأخضر والأبيض يلعبان كرة السلة مع صبيان يرتديان الزي الأبيض والأزرق
\
In the second sentence, we show another example of a grammatical error regarding a mismatch of the singular and plural forms. The first word "لاعبي", which means a group of players, is plural, while the verb in the middle of the sentence "يعالج" is singular. In this case, the correction is to modify the singular or plural based on the image described in the database. Also, the word "لاعبي" in the nominative case is written "لاعبو". Thus, the correct caption is as follows:
\
لاعبو كرة قدم يعالجون مشكلة مع لاعب كرة قدم آخر
\
In the third sentence, we show an example of an erroneous caption with two different types of errors, i.e., grammatical and spelling errors. The first is the grammatical error in the word’s nominative, accusative, and prepositional cases. In this case, the word "كلبين" is in the nominative case, and the correct way to write it is "كلبان". As for the spelling error, it is shown in "علع مدمار". The correction is "على مضمار". Table 6 displays some spelling errors, its correction, and its translation in English. These spelling errors increased the number of unique words in the dataset and impacted the trained model negatively.

**Building the Flickr8k-Rev dataset.** When building the Flickr8k-Rev dataset, we developed a GUI-based application using Guizero [41] to facilitate writing captions for each image. The user interface displays the image and provides text boxes to add the five captions to the displayed image, save it with the click of a button, and then move to the next image with the “Next” button. Figure 4 shows a screenshot of the GUI. A user interface was created that enables us to see the current image and add five captions to it, and then press the “Save” button to save each of the five captions along with the path of the image in a separate line in a file and then users can press the “Next” button to move to the following image.
\
Table 7 presents the dataset specifications. Our Flickr8k-Rev dataset contains 7,684 unique vocabularies, and the length of each caption ranges from 2 to 19. The mean, variance, and standard deviation values of the length of captions are 6.1152, 3.7905, and 1.9469, respectively. In Figure 5, we show the distribution of the caption lengths in Flickr8k-Rev. Captions in our dataset are verified by printing all unique vocabulary and ensuring they are free from spelling errors, wrongly repeated letters, single letters, extra spaces, and missing captions.
\
## Prototype Implementation
The implementation architecture is composed of a two-step pipeline, as shown in Figure 6:
1. After extracting the image features from the CNN pre-trained model, the encoder receives an image’s feature vector.
2. The decoder takes the sequence vector after it is preprocessed along with the image representation (the output from the encoder) and then extracts the sequence output.
\
As shown in Figure 6, we must prepare the dataset containing images and text before training the model. In our experiment, we use Arabic Flickr8k-Rev and the Arabic Flickr8k datasets, as explained in the previous section. Moreover, we preprocess the images and text, as described below.

### Image Preprocessing
The dataset contains different sizes of RGB images. The following image preprocessing steps are used to preprocess the images.
- Decode a JPEG-encoded image to a uint8 tensor and three color channels.
- Resize all images to the same size, 299x299 pixels.
- Convert the image to dtype, normalized to floating point values in the range [0, 1).
- Pass the resulting tensors through the CNN model, where the output layer is the last convolutional layer to extract the feature vector.

We use pre-trained CNN models without the fully connected layers for feature extraction. In our prototype, we use the ResNet-152 [15] and the EfficientNet model [16]. Next, give a brief description of these two models.
\
**ResNet-152 [15].** By learning the residual representation functions rather than the signal representation directly, ResNet creates an intense network with up to 152 layers. To fit the input from the previous layer to the next layer without modifying the input, ResNet introduces the idea of skipping connections which leads to deeper networks.
**EfficientNet model [16].** The EfficientNet model is a neural search that uses a compound scaling method with a compound coefficient to optimize accuracy and efficiency. The mobile-size-baseline EfficientNet helps optimize the accuracy and efficiency of classification, model scaling, and identifying balancing network depth, width, and resolution. Data augmentation will also be applied to increase the number of data and rotation in different angles to avoid the overfitting problem and give a better generalization. The authors created a family of EfficientNets from B0 to B7, which achieved state-of-the-art accuracy on ImageNet while being very efficient compared to its competitors. B0 is used as a baseline in their work.

### Text Preprocessing

The Arabic text goes through several preprocessing steps, as shown in Figure 7. We defined the following three mechanisms for preprocessing captions.
**Mechanism 1: (CAMeL)** The first mechanism uses only CAMeL Tools functions. Some of its functions are used in the following preprocessing steps: orthographic normalization, dediacritization, and morphological tokenization.
**Mechanism 2: (Arabert)** The second mechanism is based on ArabertPreprocessor, and its functions used for the following preprocessing steps: tatweel removal, dediacritization, and morphological tokenization.
**Mechanism 3: (Stanza)** The last mechanism is based on Stanza in the morphological tokenization step. Stanza does not have functions that can be used in other steps.
\
Some steps are common to all mechanisms, such as text cleaning and non-Arabic letter removal. Text cleaning is the step mentioned in GPP and is applied similarly in the three mechanisms. Shoukry et al. [34] method is followed in non-Arabic letters removal for all three mechanisms, orthographic normalization for the second and third mechanisms, and de-diacritization for the third. Tatweel removal has been implemented in a second mechanism only. The TSPP steps are applied similarly for all three mechanisms: the addition of <start> and <end> tokens and text vectorization.

### Datasets and Training Methodology

In our study, we use a splitting ratio of 80%,10%, and 10% for training, validation, and testing, respectively. First, we divide the two datasets into two partitions with a ratio of 90% and 10%. We divide the partition containing the 90% further into two partitions such that the partition used for training is 80% of the original dataset size, and the remaining is used for validation. The partition used for testing corresponds to 10% of the original dataset size. We followed this approach to remain comparable to ElJundi et al. [28] where the dataset is split with a ratio of 90% for training and 10% for testing, but they don’t use a validation set. Moreover, this splitting approach allows us to apply the k-fold cross-validation technique, which takes a portion from the training set for validation, as shown in Table 8.

## Experimental Evaluation

In this section, we present our empirical study. We start by describing the evaluation metrics used in our experimental evaluation. Table 9 summarizes the notations used throughout this subsection.

### Evaluation Metrics

We focus on four metrics commonly used in image captioning research evaluation.
\
**BiLingual Evaluation Understudy (BLEU).** BLEU [42], which we denote as B, is a metric for evaluating machine-translated text automatically. The BLEU score is between 0 and 1, indicating how closely the machine-translated text resembles a collection of high-quality reference translations. A number of 0 indicates that the machine-translated output has no overlap with the reference translation (poor quality), while a value of 1 indicates that the overlap is excellent (high quality). It is the most frequently used metric for IC evaluation and is used to examine the n-gram correlation between the assessed translation statement and the reference translation statement.
\
**Metric for Evaluation of Translation with Explicit Ordering (METEOR).** METEOR [43], which we denote as M, is created to address various flaws with the BLEU metric. The creators of the METEOR metric claim that METEOR can capture the correlation with human judgments better than the BLEU metric. The metric also gives more focus on recall over precision. METEOR is based on unigram-precision and unigram-recall.
\
**Recall-Oriented Understudy for Gisting Evaluation (ROUGE).** ROUGE [44], which we denote as R, consists of a set of metrics for evaluating automatic text summarization and is also applied for image captioning. These metrics compare reference (human-produced) summaries or captions with summaries or captions generated automatically. It calculates the longest matching sequence of words between the generated caption and the reference caption to determine their similarity.
\
**Consensus-based Image Description Evaluation (CIDEr).** CIDEr [45], which we denote as C. CIDEr is based on computing the cosine similarity between the candidate caption and the set of reference captions associated with the image’s Term Frequency-Inverse Document Frequency weighted n-grams, thus accounting for both precision and recall.

### Performance Analysis Experiments

We designed several experiments to study the factors that impact AIC performance. Our experimental study concerns several factors, such as the datasets, the preprocessing mechanism, and the image feature extractors.
\
We use the Flickr8k and the Flickr8k-Rev datasets to study the impact of the datasets. The preprocessing mechanisms are described in Section 7. In addition, we also include a baseline mechanism (Mechanism 0) that does not include LSPP operations and uses only the GPP and TSPP operations. We refer to this approach as GTSPP. EfficientNet-B0 and ResNet152 are used as image feature extractors.
\
Experiments are carried out in Colab Pro+ with a GPU and a high RAM. The models are trained using ten-fold cross-validation. The hyperparameters set in the model were as follows:
the batch size is 64, the number of epochs is 30 with early stopping, the patience parameter is set to 3, and the learning-rate parameter is set to 1e-4. For all experiments, we use the cross-entropy loss function and Adam optimizer. Table 10 presents a numerical summary of our experimental study.
\
**Impact of the training Dataset.** We run two experiments with GTSPP using Flickr8k and Flickr8k-Rev to understand the impact of using the different datasets for training the model. After conducting experiments using the two datasets, it turned out that using Flickr8k-Rev improves the quality of the captions, as shown in Figure 8 and 9. Figure 8 shows BLEU-n scores for the two datasets for ten-fold cross-validation where the BLEU-1 (B1) values for the dataset Flickr8k dataset range from 0.219 - 0.359 and BLEU-4 (B4) values range from 0.034 - 0.065. Flickr8k-Rev dataset BLEU-1 (B1) ranges from 0.404 - 0.449, and BLEU-4 (B4) ranges from 0.112 - 0.139 for the testing set. After using the Flickr8k-Rev the results is improved by 18.4% - 9.05% for BLEU-1 and 7.75% - 7.48% for BLEU-4. Figure 9 shows the result for M, R, and C scores. The M values for the Flickr8k dataset range from 0.223 - 0.24, R values range from 0.245 - 0.304, and C values range from 0.187 - 0.37. Flickr8k-Rev dataset M values range from 0.277 - 0.289, R values range from 0.385 - 0.41, and C values range from 0.451 - 0.53.
\
**Impact of Image Feature Extraction Models.** In the following experiment, we study the impact of the model used for image feature extraction on the generated captions. We use two image feature extraction models, EfficientNet-B0 and ResNet152. The shape of the feature vector extracted from EfficientNet-B0 and ResNet152 are 81 × 1280 and 100 × 2048, respectively. The average performance of the model that uses ResNet152 is better than the model that uses EfficientNet-B0, as shown in Figure 10 and 11. Figure 10, shows the result for B1, B2, B3, and B4 scores. The BLEU average values of the ten-fold cross-validation of ResNet152 are 0.329, 0.198, 0.108, and 0.057 for B1, B2, B3, and B4, respectively. The EfficientNet-B0 scores are 0.295, 0.179, 0.099, and 0.052 for B1, B2, B3,
and B4, respectively. In Figure 11, the average scores for M, R, and C of the model using ResNet152 are 0.224, 0.288, and 0.34, respectively. While the scores for EfficientNet-B0 are 0.231, 0.28, and 0.289 for M, R, and C, respectively. We note that all ResNet152 results are higher than EfficientNet-B0 except for R. EfficientNet-B0 is more complex than ResNet152, so it has more variations in data and is more prone to overfitting due to regularization.
\
**Impact of Text Preprocessing.** We experimentally study the four aforementioned preprocessing mechanisms. The mechanisms are evaluated to analyze their impact and importance in generating accurate captions. The different preprocessing mechanisms have an impact on the number of unique vocabulary. In Table 11, we show the number of unique vocabulary after applying each mechanism in Flickr8k and Flickr8k-Rev. The vocabulary size is decreased in the two datasets after using the preprocessing mechanisms on the text. The smallest vocabulary size is with the CAMeL mechanism. GTSPP is used as a baseline for our quantitative comparison with the rest of the preprocessing mechanisms to study their impact on generating accurate captions. Figure 12 shows the values of B1, B2, B3, B4, M, R, and C, for the Flickr8k dataset. Generally, we note the better performance results when applying the preprocessing mechanisms CAMeL, Arabert, and Stanza. Compared to GTSPP, using CAMeL improves the performance by 0.09-0.16. Using Arabert compared to GTSPP enhances the performance by 0.05-11. For Stanza, the performance improves by 0.003- 0.013 except for the B2 and the C scores, which decreases by 0.0004 and 0.011, respectively. Similar behavior is noted for the Flickr8k-Rev dataset. Compared to GTSPP, the improvements are 0.10-0.28, 0.05 to 0.09, and 0.0009 to 0.009 for CAMeL and Arabert. For Stanza, the improvement is not significant (i.e., 0.0009-0.009). The improvement results can be attributed to the tokenization operation in camel because it splits pronouns related to the word while Arabert and Stanza don’t, as we noticed in Table 4. This means that CAMeL extracts the root of the word better and thus facilitates the complexity of the Arabic. So, appropriate preprocessing significantly affects the performance and the quality of the resulting captions.
\
**Comparison with the previous studies.** Our best-performing model (using ResNet152 and CAMeL preprocessing) is compared with previous studies ElJundi et al. [28], Hejazi et al. [29], and Emami et al. [30]. In all previous works, the Arabic Flickr8k dataset is used for training their models. Table 12 summarizes all the results for all metrics in our study. Figure 14 show the comparison results of B1, B2, B3, and B4 scores. Using the Flickr8k dataset, our model scored 0.4894, 0.317, 0.213, and 0.145 for B1, B2, B3, and B4, respectively. We improve the result using the same dataset by 25-47%, 29-64%, 42-103%, and 58-154%, for B1, B2, B3, and B4, respectively. Using the Flickr8k-Rev dataset, our model significantly improved over previous studies and scores 0.616, 0.47, 0.359, and 0.273 for B1, B2, B3, and B4, respectively. This is an performance improvement of 58-86%, 91-144%, 139-242%, and 197-379%, for B1, B2, B3, and B4, respectively. The above results show that our proposed model can achieve better results up to an order of magnitude.
\
**Qualitative Results.** Figure 15 shows three sample images from the test set. The captions labeled with 1 are generated using the Flickr8k-Rev dataset, while the captions labeled with 2 are generated using the Flickr8k dataset. In both cases, we used CAMeL Tools as the preprocessor. In all three examples, the captions with 1 labels are more accurate and have fewer errors. In Figure 15a, the second sentence suffers from additional "ة" characters in the last word. The second caption in Figure 15b has a grammatical error. Finally, in Figure 15c, the first caption also describes the color of the dog as "بني", which means brown. Because Flickr8k has some spelling and grammatical errors, these errors are carried over to the results.

## Conclusion

This paper systematically analyzes the performance of transformer-based image captioning models for Arabic captioning. We study the impact of text preprocessing, image feature extractors, and datasets on the performance of AIC models. Our study shows that using CAMeL Tools to preprocess text labels improves the AIC performance by up to 34-92% in the BLEU-4 score. In addition, we study the impact of image recognition models. Our results show that ResNet152 is better than EfficientNet-B0 and can improve BLEU scores performance by 9-11%. Furthermore, we investigate the impact of different datasets on the overall AIC performance and build an extended version of the Arabic Flickr8k dataset. Using the extended version improves the BLEU-4 score of the AIC model by up to 148%. Finally, utilizing our results, we build a model that significantly outperforms the state-of-the-art proposals in AIC by up to 196-379% in the BLUE-4 score. Future work includes exploring more language-specific preprocessing techniques to improve the text preprocessing mechanisms and different architectures of transformer-based deep learning models for AIC.
