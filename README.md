# aic-flickr8k-rev
## Introduction
Image captioning is becoming increasingly important as visual media is created at an unprecedented rate on the web and social media platforms. Research on image captioning targeting English captions has received significant attention in the past decade (Kiros et al., 2014; Xu et al., 2015; Sharma et al., 2018; Cornia et al., 2020; Dubey et al., 2023). Unfortunately, image captioning for the Arabic language did not receive similar attention, and few existing works focused on Arabic image captioning (AIC). To solve the AIC problem, existing models developed for English can be used to generate captions in English, then use a text-to-text language translation model to get the captions in Arabic. However, there are many drawbacks to this approach. First, captioning inaccuracies may result from different sources, namely the captioning and translation models. In addition, using the model can be slower because it performs two steps to get the Arabic text from the image. Furthermore, generating captions in Arabic faces many challenges. The Arabic language has a vast morphology (22,400 tags compared to 48 tags in English) (Habash, 2010). Moreover, Arabic has a fairly complex inflectional system. For example, Arabic verbs have over 5,400 different inflected forms. Thus, generating meaningful Arabic captions that are grammatically correct is challenging. In addition to the challenges that stem from the inherent properties of the Arabic language, we identify three factors that impact machine learning models used for AIC. These factors are (1) the labelsâ€™ textual preprocessing techniques, (2) the datasets used for building AIC models, and (3) the techniques used to extract the features from the input images to generate the output captions.

## A Transformer-based Architecture for AIC
Proposed by Vaswani et al. (2017), the transformer model is one of the deep-learning models that can be used for machine translation. It has been used in a wide range of NLP applications Wang et al. (2019); Raganato and Tiedemann (2018). In addition to NLP applications, the transformer model has also been used for image captioning Sharma et al. (2018). We propose a variant based on Sharma et al. (2018).
Our proposed transformer-based architecture is shown in Fig. 1. The input image is translated into a vector with feature embedding using a CNN-based model. Then, the transformer encoder takes in the feature embedding of the image extracted by CNN, $x = (x_1,..., x_n)$. Thus, the image's feature vector $x$ is computed based on the equation.
\
   $x$ = $CNN(i)$
\
$CNN(i)$ in the equation represents an abstraction over any CNN-based model that is used for image feature extraction. 
